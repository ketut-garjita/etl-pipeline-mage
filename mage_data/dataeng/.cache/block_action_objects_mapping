{"block_file": {"custom/spark_session.py:custom:python:spark session": {"content": "from pyspark.conf import SparkConf\nfrom pyspark.sql import SparkSession\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n@data_loader\ndef load_data(*args, **kwargs):\n    spark = (\n        SparkSession\n        .builder\n        .appName('Test spark')\n        .getOrCreate()\n    )\n    kwargs['context']['spark'] = spark\n    ...\n", "file_path": "custom/spark_session.py", "language": "python", "type": "custom", "uuid": "spark_session"}, "data_exporters/department_dim_to_big_query.py:data_exporter:python:department dim to big query": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'supply_chain_data.stg_department'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/department_dim_to_big_query.py", "language": "python", "type": "data_exporter", "uuid": "department_dim_to_big_query"}, "data_exporters/customer_dim_to_big_query.py:data_exporter:python:customer dim to big query": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'supply_chain_data.stg_customer'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/customer_dim_to_big_query.py", "language": "python", "type": "data_exporter", "uuid": "customer_dim_to_big_query"}, "data_exporters/shipping_dim_to_big_query.py:data_exporter:python:shipping dim to big query": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'supply_chain_data.stg_shipping'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/shipping_dim_to_big_query.py", "language": "python", "type": "data_exporter", "uuid": "shipping_dim_to_big_query"}, "data_exporters/export_mart_tables_to_clickhouse.py:data_exporter:python:export mart tables to clickhouse": {"content": "import io\nimport pandas as pd\nimport requests\nimport os \nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.duckdb import DuckDB\n\n\nimport duckdb\nfrom clickhouse_driver import Client\nfrom datetime import datetime\nimport pandas as pd\nimport clickhouse_connect\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.io.clickhouse import ClickHouse\nfrom pandas import DataFrame\nfrom typing import Dict\nimport os\n\n@data_exporter\ndef export_data_to_clickhouse(data: Dict[str, DataFrame], *args, **kwargs):\n    \"\"\"\n    Export multiple tables to ClickHouse without using context manager\n    \"\"\"\n    config_path = os.path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    # Initialize ClickHouse connection directly\n    try:\n        config = ConfigFileLoader(config_path, config_profile)\n        exporter = ClickHouse(\n            database=config['CLICKHOUSE']['database'],\n            host=config['CLICKHOUSE']['host'],\n            password=config['CLICKHOUSE']['password'],\n            port=config['CLICKHOUSE']['port'],\n            user=config['CLICKHOUSE']['user'],\n        )\n        \n        for table_name, df in data.items():\n            # Convert pandas dtypes to ClickHouse types\n            schema = []\n            for col, dtype in zip(df.columns, df.dtypes):\n                ch_type = 'String'  # default\n                if 'int' in str(dtype):\n                    if df[col].max() > 2147483647:\n                        ch_type = 'Int64'\n                    else:\n                        ch_type = 'Int32'\n                elif 'float' in str(dtype):\n                    ch_type = 'Float64'\n                schema.append(f\"{col} Nullable({ch_type})\")\n            \n            # Create table if not exists\n            exporter.execute(\n                f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n                f\"{', '.join(schema)}\"\n                f\") ENGINE = MergeTree() ORDER BY tuple()\"\n            )\n            \n            # Export data (batch insert)\n            exporter.export(\n                df,\n                table_name,\n                if_exists='replace',  # or 'append' for incremental\n                chunk_size=10000  # Adjust based on your data size\n            )\n            \n    except Exception as e:\n        print(f\"Error exporting to ClickHouse: {str(e)}\")\n        raise\n    finally:\n        if 'exporter' in locals():\n            # Manually close connection if needed\n            if hasattr(exporter, 'close'):\n                exporter.close()\n            elif hasattr(exporter, '_ctx'):\n                exporter._ctx.disconnect()", "file_path": "data_exporters/export_mart_tables_to_clickhouse.py", "language": "python", "type": "data_exporter", "uuid": "export_mart_tables_to_clickhouse"}, "data_exporters/export_data_to_clickhouse.py:data_exporter:python:export data to clickhouse": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nimport pandas as pd\n\n@data_exporter\ndef export_data_to_clickhouse(df: pd.DataFrame, *args, **kwargs) -> None:\n    \"\"\"\n    Export data to ClickHouse with the specified table structure.\n    \"\"\"\n    from clickhouse_driver import Client\n    \n    # ClickHouse connection settings\n    settings = {\n        'host': 'clickhouse-server',\n        'port': '9000',\n        'user': 'etl_user',\n        'password': 'your_strong_password_123',\n        'database': 'etl'\n    }\n    \n    # Initialize ClickHouse client\n    client = Client(**settings)\n    \n    # Create table if not exists (matching DuckDB structure)\n    create_table_query = \"\"\"\n    CREATE TABLE IF NOT EXISTS mart_overall_performance (\n        year                        BigInt,\n        month_name                  String,\n        total_sales                 Float64,\n        total_profit                Float64,\n        avg_profit_margin           Float64,\n        avg_actual_shipment_days    Float64,\n        avg_scheduled_shipment_days Float64\n    ) ENGINE = MergeTree()\n    ORDER BY (year, month_name)\n    \"\"\"\n    \n    client.execute(create_table_query)\n    \n    # Prepare data for insertion\n    data = df.to_dict('records')\n    \n    # Insert data\n    client.execute(\n        \"INSERT INTO mart_overall_performance VALUES\",\n        data,\n        types_check=True\n    )\n    \n    client.disconnect()\n", "file_path": "data_exporters/export_data_to_clickhouse.py", "language": "python", "type": "data_exporter", "uuid": "export_data_to_clickhouse"}, "data_exporters/kafka_to_duckdb.yaml:data_exporter:yaml:kafka to duckdb": {"content": "connector_type: duckdb\ndatabase: /home/dataeng/projects/etl-duckdb-mage/streaming_pipeline/data/warehouse.duckdb\nschema: public\ntable_name: supply_chain_stream\nfile_type: json  # other options: csv, parquet\nmode: append\ncreate_table_if_not_exists: true\npartition_by: null\n", "file_path": "data_exporters/kafka_to_duckdb.yaml", "language": "yaml", "type": "data_exporter", "uuid": "kafka_to_duckdb"}, "data_exporters/order_dim_to_big_query.py:data_exporter:python:order dim to big query": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'supply_chain_data.stg_order'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/order_dim_to_big_query.py", "language": "python", "type": "data_exporter", "uuid": "order_dim_to_big_query"}, "data_exporters/metadata_dim_to_big_query.py:data_exporter:python:metadata dim to big query": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'supply_chain_data.stg_metadata'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/metadata_dim_to_big_query.py", "language": "python", "type": "data_exporter", "uuid": "metadata_dim_to_big_query"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/location_dim_to_big_query.py:data_exporter:python:location dim to big query": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'supply_chain_data.stg_location'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/location_dim_to_big_query.py", "language": "python", "type": "data_exporter", "uuid": "location_dim_to_big_query"}, "data_exporters/curious_sun.py:data_exporter:python:curious sun": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports data to some source.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Output (optional):\n        Optionally return any object and it'll be logged and\n        displayed when inspecting the block run.\n    \"\"\"\n    # Specify your data exporting logic here\n\n\n", "file_path": "data_exporters/curious_sun.py", "language": "python", "type": "data_exporter", "uuid": "curious_sun"}, "data_exporters/product_dim_to_big_query.py:data_exporter:python:product dim to big query": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'supply_chain_data.stg_product'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/product_dim_to_big_query.py", "language": "python", "type": "data_exporter", "uuid": "product_dim_to_big_query"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/load_customer_dimension.py:data_loader:python:load customer dimension": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.config import ConfigFileLoader, ConfigKey\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Import necessary modules\nimport pandas as pd\nfrom os import path\nfrom google.cloud import storage\n\n# Define function to list files in Google Cloud Storage bucket\ndef list_files_in_gcs_bucket(bucket_name, service_account_key_path):\n    # Initialize a client using the service account key\n    client = storage.Client.from_service_account_json(service_account_key_path)\n    \n    # Get the bucket\n    bucket = client.get_bucket(bucket_name)\n    \n    # List all blobs (files) in the bucket\n    blobs = bucket.list_blobs()\n    \n    # Extract file names from the blobs\n    file_names = [blob.name for blob in blobs]\n    \n    return file_names\n\n# Load configuration from YAML file\nconfig = ConfigFileLoader('./dataeng-2/io_config.yaml', 'default')\ngcs = config[ConfigKey.GOOGLE_SERVICE_ACC_KEY_FILEPATH]\n\n# Define bucket name\nbucket_name = 'supply-chain-data'\n\n# List all file names in the bucket\nfile_names = list_files_in_gcs_bucket(bucket_name, gcs)\n\n# Filter file names for customer dimension\nstg_customer = [file_name for file_name in file_names if file_name.startswith('transformed_data/stg_customer.parquet/part-')]\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    dfs = []\n    \n    # Iterate over file paths in customer dimension\n    for file_path in stg_customer:\n        # Define object key\n        object_key = file_path\n        \n        # Load DataFrame from Google Cloud Storage\n        df = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(bucket_name, object_key)\n        \n        # Append DataFrame to list\n        dfs.append(df)\n    \n    # Concatenate DataFrames\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_customer_dimension.py", "language": "python", "type": "data_loader", "uuid": "load_customer_dimension"}, "data_loaders/consume_from_kafka_clickhouse.yaml:data_loader:yaml:consume from kafka clickhouse": {"content": "connector_type: kafka\nbootstrap_server: kafka:29092\ntopic: supply_chain_data\nconsumer_group: mage_consumer_group\ninclude_metadata: true\napi_version: 0.10.2\nconfig:\n  connector_type: kafka\n  bootstrap_server: kafka:29092\n  topic: supply_chain_data\n  consumer_group: mage_consumer_group\n  include_metadata: true\n  api_version: 0.10.2\n", "file_path": "data_loaders/consume_from_kafka_clickhouse.yaml", "language": "yaml", "type": "data_loader", "uuid": "consume_from_kafka_clickhouse"}, "data_loaders/load_product_dimension.py:data_loader:python:load product dimension": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.config import ConfigFileLoader, ConfigKey\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Import necessary modules\nimport pandas as pd\nfrom os import path\nfrom google.cloud import storage\n\n# Define function to list files in Google Cloud Storage bucket\ndef list_files_in_gcs_bucket(bucket_name, service_account_key_path):\n    # Initialize a client using the service account key\n    client = storage.Client.from_service_account_json(service_account_key_path)\n    \n    # Get the bucket\n    bucket = client.get_bucket(bucket_name)\n    \n    # List all blobs (files) in the bucket\n    blobs = bucket.list_blobs()\n    \n    # Extract file names from the blobs\n    file_names = [blob.name for blob in blobs]\n    \n    return file_names\n\n# Load configuration from YAML file\nconfig = ConfigFileLoader('./dataeng-2/io_config.yaml', 'default')\ngcs = config[ConfigKey.GOOGLE_SERVICE_ACC_KEY_FILEPATH]\n\n# Define bucket name\nbucket_name = 'supply-chain-data'\n\n# List all file names in the bucket\nfile_names = list_files_in_gcs_bucket(bucket_name, gcs)\n\n# Filter file names for customer dimension\nstg_customer = [file_name for file_name in file_names if file_name.startswith('transformed_data/stg_product.parquet/part-')]\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    dfs = []\n    \n    # Iterate over file paths in customer dimension\n    for file_path in stg_customer:\n        # Define object key\n        object_key = file_path\n        \n        # Load DataFrame from Google Cloud Storage\n        df = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(bucket_name, object_key)\n        \n        # Append DataFrame to list\n        dfs.append(df)\n    \n    # Concatenate DataFrames\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_product_dimension.py", "language": "python", "type": "data_loader", "uuid": "load_product_dimension"}, "data_loaders/load_department_dimension.py:data_loader:python:load department dimension": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.config import ConfigFileLoader, ConfigKey\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Import necessary modules\nimport pandas as pd\nfrom os import path\nfrom google.cloud import storage\n\n# Define function to list files in Google Cloud Storage bucket\ndef list_files_in_gcs_bucket(bucket_name, service_account_key_path):\n    # Initialize a client using the service account key\n    client = storage.Client.from_service_account_json(service_account_key_path)\n    \n    # Get the bucket\n    bucket = client.get_bucket(bucket_name)\n    \n    # List all blobs (files) in the bucket\n    blobs = bucket.list_blobs()\n    \n    # Extract file names from the blobs\n    file_names = [blob.name for blob in blobs]\n    \n    return file_names\n\n# Load configuration from YAML file\nconfig = ConfigFileLoader('./dataeng-2/io_config.yaml', 'default')\ngcs = config[ConfigKey.GOOGLE_SERVICE_ACC_KEY_FILEPATH]\n\n# Define bucket name\nbucket_name = 'supply-chain-data'\n\n# List all file names in the bucket\nfile_names = list_files_in_gcs_bucket(bucket_name, gcs)\n\n# Filter file names for customer dimension\nstg_customer = [file_name for file_name in file_names if file_name.startswith('transformed_data/stg_department.parquet/part-')]\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    dfs = []\n    \n    # Iterate over file paths in customer dimension\n    for file_path in stg_customer:\n        # Define object key\n        object_key = file_path\n        \n        # Load DataFrame from Google Cloud Storage\n        df = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(bucket_name, object_key)\n        \n        # Append DataFrame to list\n        dfs.append(df)\n    \n    # Concatenate DataFrames\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_department_dimension.py", "language": "python", "type": "data_loader", "uuid": "load_department_dimension"}, "data_loaders/load_duckdb_mart_tables.py:data_loader:python:load duckdb mart tables": {"content": "import io\nimport pandas as pd\nimport requests\nimport os \nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.duckdb import DuckDB\nfrom mage_ai.io.clickhouse import ClickHouse\nfrom pandas import DataFrame\nfrom typing import Dict\nimport duckdb\nfrom clickhouse_driver import Client\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_duckdb(*args, **kwargs) -> Dict[str, DataFrame]:\n    \"\"\"\n    Direct DuckDB connection without Mage wrapper\n    \"\"\"\n    duckdb_path = \"/home/src/dataeng-2/dbt/warehouse.duckdb\"  # Update this path\n    \n    try:\n        conn = duckdb.connect(database=duckdb_path, read_only=False)\n        \n        tables = [\n            'mart_customer_retention_rate',\n            'mart_financial_commitments',\n            'mart_fraud_detection',\n            'mart_inventory_levels',\n            'mart_payment_delays'\n        ]\n        \n        return {\n            table: conn.execute(f\"SELECT * FROM {table}\").fetchdf()\n            for table in tables\n        }\n        \n    except Exception as e:\n        print(f\"DuckDB Error: {str(e)}\")\n        raise\n    finally:\n        if 'conn' in locals():\n            conn.close()\n", "file_path": "data_loaders/load_duckdb_mart_tables.py", "language": "python", "type": "data_loader", "uuid": "load_duckdb_mart_tables"}, "data_loaders/frosty_sorcerer.py:data_loader:python:frosty sorcerer": {"content": "import requests\nfrom io import BytesIO\nimport PyPDF2\nfrom datetime import datetime\n\n@data_loader\ndef load_pdf_from_github(**kwargs):\n\n    # Load PDF from GitHub repository\n    github_pdf_url = \"<https://raw.githubusercontent.com/mage-ai/datasets/master/great_attractor_pdf.pdf>\"\n    \n    print(f\"Fetching PDF from GitHub: {github_pdf_url}\")\n    \n    # Download the PDF\n    try:\n        response = requests.get(github_pdf_url)\n        response.raise_for_status()  # Ensure we got a valid response\n        \n        # Read the PDF content\n        pdf_content = BytesIO(response.content)\n        pdf_reader = PyPDF2.PdfReader(pdf_content)\n        \n        # Extract metadata\n        pdf_info = {\n            \"title\": \"Evidence of the Great Attractor and Great Repeller\",\n            \"author\": \"Christopher C. O'Neill\",\n            \"num_pages\": len(pdf_reader.pages),\n            \"source_url\": github_pdf_url,\n            \"fetch_time\": datetime.now().isoformat()\n        }\n        \n        # Extract text from all pages\n        all_text = \"\"\n        for page_num in range(len(pdf_reader.pages)):\n            page = pdf_reader.pages[page_num]\n            page_text = page.extract_text()\n            all_text += page_text + \"\\\\n\\\\n\"\n        \n        print(f\"Successfully extracted {pdf_info['num_pages']} pages from PDF\")\n        \n        # Create result with both metadata and content\n        result = {\n            \"metadata\": pdf_info,\n            \"content\": all_text\n        }\n        \n        return result\n    except Exception as e:\n        print(f\"Error fetching PDF from GitHub: {e}\")\n        # Instead of providing sample text, let's throw an error to ensure we don't proceed without the actual document\n        raise Exception(f\"Failed to load PDF document: {e}\")", "file_path": "data_loaders/frosty_sorcerer.py", "language": "python", "type": "data_loader", "uuid": "frosty_sorcerer"}, "data_loaders/load_order_dimension.py:data_loader:python:load order dimension": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.config import ConfigFileLoader, ConfigKey\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Import necessary modules\nimport pandas as pd\nfrom os import path\nfrom google.cloud import storage\n\n# Define function to list files in Google Cloud Storage bucket\ndef list_files_in_gcs_bucket(bucket_name, service_account_key_path):\n    # Initialize a client using the service account key\n    client = storage.Client.from_service_account_json(service_account_key_path)\n    \n    # Get the bucket\n    bucket = client.get_bucket(bucket_name)\n    \n    # List all blobs (files) in the bucket\n    blobs = bucket.list_blobs()\n    \n    # Extract file names from the blobs\n    file_names = [blob.name for blob in blobs]\n    \n    return file_names\n\n# Load configuration from YAML file\nconfig = ConfigFileLoader('./dataeng-2/io_config.yaml', 'default')\ngcs = config[ConfigKey.GOOGLE_SERVICE_ACC_KEY_FILEPATH]\n\n# Define bucket name\nbucket_name = 'supply-chain-data'\n\n# List all file names in the bucket\nfile_names = list_files_in_gcs_bucket(bucket_name, gcs)\n\n# Filter file names for customer dimension\nstg_customer = [file_name for file_name in file_names if file_name.startswith('transformed_data/stg_order.parquet/part-')]\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    dfs = []\n    \n    # Iterate over file paths in customer dimension\n    for file_path in stg_customer:\n        # Define object key\n        object_key = file_path\n        \n        # Load DataFrame from Google Cloud Storage\n        df = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(bucket_name, object_key)\n        \n        # Append DataFrame to list\n        dfs.append(df)\n    \n    # Concatenate DataFrames\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_order_dimension.py", "language": "python", "type": "data_loader", "uuid": "load_order_dimension"}, "data_loaders/consume_from_kafka.yaml:data_loader:yaml:consume from kafka": {"content": "connector_type: kafka\nbootstrap_server: \"kafka:29092\"\ntopic: supply_chain_data\nconsumer_group: mage_consumer_group\ninclude_metadata: true\napi_version: \"0.10.2\"\nauto_offset_reset: \"earliest\"  # This is the correct parameter name\n", "file_path": "data_loaders/consume_from_kafka.yaml", "language": "yaml", "type": "data_loader", "uuid": "consume_from_kafka"}, "data_loaders/load_location_dimension.py:data_loader:python:load location dimension": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.config import ConfigFileLoader, ConfigKey\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Import necessary modules\nimport pandas as pd\nfrom os import path\nfrom google.cloud import storage\n\n# Define function to list files in Google Cloud Storage bucket\ndef list_files_in_gcs_bucket(bucket_name, service_account_key_path):\n    # Initialize a client using the service account key\n    client = storage.Client.from_service_account_json(service_account_key_path)\n    \n    # Get the bucket\n    bucket = client.get_bucket(bucket_name)\n    \n    # List all blobs (files) in the bucket\n    blobs = bucket.list_blobs()\n    \n    # Extract file names from the blobs\n    file_names = [blob.name for blob in blobs]\n    \n    return file_names\n\n# Load configuration from YAML file\nconfig = ConfigFileLoader('./dataeng-2/io_config.yaml', 'default')\ngcs = config[ConfigKey.GOOGLE_SERVICE_ACC_KEY_FILEPATH]\n\n# Define bucket name\nbucket_name = 'supply-chain-data'\n\n# List all file names in the bucket\nfile_names = list_files_in_gcs_bucket(bucket_name, gcs)\n\n# Filter file names for customer dimension\nstg_customer = [file_name for file_name in file_names if file_name.startswith('transformed_data/stg_location.parquet/part-')]\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    dfs = []\n    \n    # Iterate over file paths in customer dimension\n    for file_path in stg_customer:\n        # Define object key\n        object_key = file_path\n        \n        # Load DataFrame from Google Cloud Storage\n        df = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(bucket_name, object_key)\n        \n        # Append DataFrame to list\n        dfs.append(df)\n    \n    # Concatenate DataFrames\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_location_dimension.py", "language": "python", "type": "data_loader", "uuid": "load_location_dimension"}, "data_loaders/load_data_from_duckdb.py:data_loader:python:load data from duckdb": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport duckdb\nimport pandas as pd\nimport os \n\n@data_loader\ndef load_data_from_duckdb(*args, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Load data from DuckDB database.\n    \"\"\"\n    import duckdb\n    \n    query = \"\"\"\n    SELECT \n        year,\n        month_name,\n        total_sales,\n        total_profit,\n        avg_profit_margin,\n        avg_actual_shipment_days,\n        avg_scheduled_shipment_days\n    FROM mart_overall_performance\n    \"\"\"\n    \n    # Connect to DuckDB and execute query\n    conn = duckdb.connect(\"/home/src/dataeng-2/dbt/warehouse.duckdb\")\n    df = conn.execute(query).fetchdf()\n    conn.close()\n    \n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_data_from_duckdb.py", "language": "python", "type": "data_loader", "uuid": "load_data_from_duckdb"}, "data_loaders/load_metadata_dimensio.py:data_loader:python:load metadata dimensio": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.config import ConfigFileLoader, ConfigKey\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Import necessary modules\nimport pandas as pd\nfrom os import path\nfrom google.cloud import storage\n\n# Define function to list files in Google Cloud Storage bucket\ndef list_files_in_gcs_bucket(bucket_name, service_account_key_path):\n    # Initialize a client using the service account key\n    client = storage.Client.from_service_account_json(service_account_key_path)\n    \n    # Get the bucket\n    bucket = client.get_bucket(bucket_name)\n    \n    # List all blobs (files) in the bucket\n    blobs = bucket.list_blobs()\n    \n    # Extract file names from the blobs\n    file_names = [blob.name for blob in blobs]\n    \n    return file_names\n\n# Load configuration from YAML file\nconfig = ConfigFileLoader('./dataeng-2/io_config.yaml', 'default')\ngcs = config[ConfigKey.GOOGLE_SERVICE_ACC_KEY_FILEPATH]\n\n# Define bucket name\nbucket_name = 'supply-chain-data'\n\n# List all file names in the bucket\nfile_names = list_files_in_gcs_bucket(bucket_name, gcs)\n\n# Filter file names for customer dimension\nstg_customer = [file_name for file_name in file_names if file_name.startswith('transformed_data/stg_metadata.parquet/part-')]\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    dfs = []\n    \n    # Iterate over file paths in customer dimension\n    for file_path in stg_customer:\n        # Define object key\n        object_key = file_path\n        \n        # Load DataFrame from Google Cloud Storage\n        df = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(bucket_name, object_key)\n        \n        # Append DataFrame to list\n        dfs.append(df)\n    \n    # Concatenate DataFrames\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_metadata_dimensio.py", "language": "python", "type": "data_loader", "uuid": "load_metadata_dimensio"}, "data_loaders/load_shipping_dimension.py:data_loader:python:load shipping dimension": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.config import ConfigFileLoader, ConfigKey\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Import necessary modules\nimport pandas as pd\nfrom os import path\nfrom google.cloud import storage\n\n# Define function to list files in Google Cloud Storage bucket\ndef list_files_in_gcs_bucket(bucket_name, service_account_key_path):\n    # Initialize a client using the service account key\n    client = storage.Client.from_service_account_json(service_account_key_path)\n    \n    # Get the bucket\n    bucket = client.get_bucket(bucket_name)\n    \n    # List all blobs (files) in the bucket\n    blobs = bucket.list_blobs()\n    \n    # Extract file names from the blobs\n    file_names = [blob.name for blob in blobs]\n    \n    return file_names\n\n# Load configuration from YAML file\nconfig = ConfigFileLoader('./dataeng-2/io_config.yaml', 'default')\ngcs = config[ConfigKey.GOOGLE_SERVICE_ACC_KEY_FILEPATH]\n\n# Define bucket name\nbucket_name = 'supply-chain-data'\n\n# List all file names in the bucket\nfile_names = list_files_in_gcs_bucket(bucket_name, gcs)\n\n# Filter file names for customer dimension\nstg_customer = [file_name for file_name in file_names if file_name.startswith('transformed_data/stg_shipping.parquet/part-')]\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    dfs = []\n    \n    # Iterate over file paths in customer dimension\n    for file_path in stg_customer:\n        # Define object key\n        object_key = file_path\n        \n        # Load DataFrame from Google Cloud Storage\n        df = GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(bucket_name, object_key)\n        \n        # Append DataFrame to list\n        dfs.append(df)\n    \n    # Concatenate DataFrames\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_shipping_dimension.py", "language": "python", "type": "data_loader", "uuid": "load_shipping_dimension"}, "dbts/knightly_pond.yaml:dbt:yaml:knightly pond": {"content": "command: run\nselect: staging.*\nproject_dir: /app/business_transformations\nvars: {\"env\": \"dev\"}\nprofiles_dir: /app/business_transformations", "file_path": "dbts/knightly_pond.yaml", "language": "yaml", "type": "dbt", "uuid": "knightly_pond"}, "dbts/green_myth.yaml:dbt:yaml:green myth": {"content": "--select staging\n--select dwh\n--select marts", "file_path": "dbts/green_myth.yaml", "language": "yaml", "type": "dbt", "uuid": "green_myth"}, "dbts/heroic_destiny.yaml:dbt:yaml:heroic destiny": {"content": "", "file_path": "dbts/heroic_destiny.yaml", "language": "yaml", "type": "dbt", "uuid": "heroic_destiny"}, "transformers/insert__raws_tables_duckdb.py:transformer:python:insert  raws tables duckdb": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.bigquery import BigQuery\nfrom os import path\nfrom pandas import DataFrame\nimport pyspark \nimport os \n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef transform_in_bigquery(*args, **kwargs) -> DataFrame:\n    import sys\n    import os\n\n    # Tambahkan path untuk bisa import script eksternal\n    sys.path.append('/home/src/dataeng-2/shared/scripts')\n    sys.path.append('/home/src/dataeng-2/shared/config')\n    sys.path.append('/home/src/dataeng-2/shared/scripts/utils')\n\n    from duckdb_raws import run_duckdb_transformation\n\n    # Jalankan script OLAP dan ambil salah satu dataframe\n    dataframes = run_duckdb_transformation()  # \u2190 pastikan fungsi ini mengembalikan dict DataFrame\n    return dataframes\n\n", "file_path": "transformers/insert__raws_tables_duckdb.py", "language": "python", "type": "transformer", "uuid": "insert__raws_tables_duckdb"}, "transformers/amazing_noble.py:transformer:python:amazing noble": {"content": "import re\n\n@transformer\ndef clean_document(data, **kwargs):\n    \"\"\"\n    Clean and format the extracted PDF text\n    \"\"\"\n    # Check if data is available\n    if data is None or \"content\" not in data:\n        raise Exception(\"No PDF content available to clean\")\n    \n    # Extract text and metadata from previous block\n    text = data[\"content\"]\n    metadata = data[\"metadata\"]\n    \n    print(\"Cleaning document text...\")\n    \n    # Clean up common PDF extraction issues\n    # 1. Replace multiple newlines with a single newline\n    text = re.sub(r'\\\\n{3,}', '\\\\n\\\\n', text)\n    \n    # 2. Fix broken words that might have been split across lines\n    text = re.sub(r'(\\\\w+)-\\\\n(\\\\w+)', r'\\\\1\\\\2', text)\n    \n    # 3. Remove headers and footers that repeat on each page\n    text = re.sub(r'Journal of High Energy Physics,\\\\s+Gravitation and Cosmology.+?\\\\d+', '', text)\n    text = re.sub(r'DOI:.+?\\\\d+', '', text)\n    \n    # 4. Normalize whitespace\n    text = re.sub(r' {2,}', ' ', text)\n    text = text.strip()\n    \n    # Add document information at the beginning\n    title_header = f\"# {metadata['title']}\\\\n\"\n    author_header = f\"## By {metadata['author']}\\\\n\"\n    source_header = f\"Source: {metadata['source_url']}\\\\n\\\\n\"\n    \n    formatted_text = title_header + author_header + source_header + text\n    \n    # Return cleaned document with metadata\n    result = {\n        \"text\": formatted_text,\n        \"metadata\": metadata,\n        \"character_count\": len(formatted_text),\n        \"processing_time\": kwargs.get(\"execution_date\", \"Unknown\")\n    }\n    \n    print(f\"Document cleaned: {len(formatted_text)} characters\")\n    return result", "file_path": "transformers/amazing_noble.py", "language": "python", "type": "transformer", "uuid": "amazing_noble"}, "transformers/create_table_2.py:transformer:python:create table 2": {"content": "from clickhouse_driver import Client\nimport pandas as pd\nfrom datetime import datetime\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n@transformer\ndef transform(data, *args, **kwargs) -> pd.DataFrame:\n    # Konversi input ke DataFrame jika perlu\n    if isinstance(data, list):\n        df = pd.DataFrame(data)\n    else:\n        df = data.copy()\n\n    # Konversi tipe data dan format tanggal\n    df['order date (DateOrders)'] = pd.to_datetime(df['order date (DateOrders)'], format='%m/%d/%Y %H:%M')\n    df['shipping date (DateOrders)'] = pd.to_datetime(df['shipping date (DateOrders)'], format='%m/%d/%Y %H:%M')\n    \n    # Untuk kolom numeric yang masih berupa string\n    numeric_cols = [\n        'Days for shipping (real)', 'Days for shipment (scheduled)',\n        'Benefit per order', 'Sales per customer', 'Late_delivery_risk',\n        'Category Id', 'Customer Id', 'Department Id', 'Order Customer Id',\n        'Order Id', 'Order Item Cardprod Id', 'Order Item Discount',\n        'Order Item Discount Rate', 'Order Item Id', 'Order Item Product Price',\n        'Order Item Profit Ratio', 'Order Item Quantity', 'Sales',\n        'Order Item Total', 'Order Profit Per Order', 'Product Card Id',\n        'Product Category Id', 'Product Price', 'Product Status'\n    ]\n    \n    for col in numeric_cols:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n\n    con = Client(\n        host='clickhouse-server',\n        port=8123,\n        user='etl_user',\n        password='your_strong_password_123',\n        database='etl'\n    )\n\n    table_name = 'supply_chain_stream'\n\n    # Buat tabel dengan skema yang sesuai\n    con.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            `Type` String,\n            `Days for shipping (real)` UInt8,\n            `Days for shipment (scheduled)` UInt8,\n            `Benefit per order` Float32,\n            `Sales per customer` Float32,\n            `Delivery Status` String,\n            `Late_delivery_risk` UInt8,\n            `Category Id` UInt32,\n            `Category Name` String,\n            `Customer City` String,\n            `Customer Country` String,\n            `Customer Email` String,\n            `Customer Fname` String,\n            `Customer Id` UInt32,\n            `Customer Lname` String,\n            `Customer Password` String,\n            `Customer Segment` String,\n            `Customer State` String,\n            `Customer Street` String,\n            `Customer Zipcode` String,\n            `Department Id` UInt32,\n            `Department Name` String,\n            `Latitude` Float64,\n            `Longitude` Float64,\n            `Market` String,\n            `Order City` String,\n            `Order Country` String,\n            `Order Customer Id` UInt32,\n            `order date (DateOrders)` DateTime,\n            `Order Id` UInt32,\n            `Order Item Cardprod Id` UInt32,\n            `Order Item Discount` Float32,\n            `Order Item Discount Rate` Float32,\n            `Order Item Id` UInt32,\n            `Order Item Product Price` Float32,\n            `Order Item Profit Ratio` Float32,\n            `Order Item Quantity` UInt8,\n            `Sales` Float32,\n            `Order Item Total` Float32,\n            `Order Profit Per Order` Float32,\n            `Order Region` String,\n            `Order State` String,\n            `Order Status` String,\n            `Order Zipcode` String,\n            `Product Card Id` UInt32,\n            `Product Category Id` UInt32,\n            `Product Description` String,\n            `Product Image` String,\n            `Product Name` String,\n            `Product Price` Float32,\n            `Product Status` UInt8,\n            `shipping date (DateOrders)` DateTime,\n            `Shipping Mode` String,\n            \n            -- Kafka metadata fields\n            `metadata__topic` String,\n            `metadata__partition` UInt32,\n            `metadata__offset` UInt64,\n            `metadata__timestamp` DateTime\n        ) ENGINE = MergeTree()\n        ORDER BY (`order date (DateOrders)`, `Order Id`, `Order Item Id`)\n    \"\"\")\n\n    # Konversi data ke format yang sesuai untuk ClickHouse\n    records = df.to_dict('records')\n    \n    # Insert data\n    con.execute(\n        f\"INSERT INTO {table_name} VALUES\",\n        records,\n        types_check=True\n    )\n\n    con.close()\n    return df", "file_path": "transformers/create_table_2.py", "language": "python", "type": "transformer", "uuid": "create_table_2"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_age = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_age)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/creative_glade.py:transformer:python:creative glade": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n    \nfrom datetime import datetime\nimport pandas as pd\nimport clickhouse_connect\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n@transformer\ndef transform(data, *args, **kwargs) -> pd.DataFrame:\n\n    # Ensure DataFrame is flattened\n    if isinstance(data, list):\n        df = pd.DataFrame(data)\n    else:\n        df = data.copy()\n    \n    # Convert the column names to a list using .tolist()\n    \n    # df = df[column_list]\n\n    try:\n        # Koneksi ke ClickHouse\n        client = clickhouse_connect.get_client(\n            host='clickhouse-server',\n            port=8123,\n            username='etl_user',\n            password='your_strong_password_123',\n            database='etl'\n        )\n\n        table_name = 'supply_chain_stream'\n\n        create_table = \"\"\"\n       CREATE TABLE supply_chain_stream (\n            data Tuple(\n                Type String,\n                `Days_for_shipping_real` String,\n                `Days_for_shipment_scheduled` String,\n                `Benefit_per_order` String,\n                `Sales_per_customer` String,\n                `Delivery_Status` String,\n                `Late_delivery_risk` String,\n                `Category_Id` String,\n                `Category_Name` String,\n                `Customer_City` String,\n                `Customer_Country` String,\n                `Customer_Email` String,\n                `Customer_Fname` String,\n                `Customer_Id` String,\n                `Customer_Lname` String,\n                `Customer_Password` String,\n                `Customer_Segment` String,\n                `Customer_State` String,\n                `Customer_Street` String,\n                `Customer_Zipcode` String,\n                `Department_Id` String,\n                `Department_Name` String,\n                Latitude String,\n                Longitude String,\n                Market String,\n                `Order_City` String,\n                `Order_Country` String,\n                `Order_Customer_Id` String,\n                `Order_Date_DateOrders` String,\n                `Order_Id` String,\n                `Order_Item_Cardprod_Id` String,\n                `Order_Item_Discount` String,\n                `Order_Item_Discount_Rate` String,\n                `Order_Item_Id` String,\n                `Order_Item_Product_Price` String,\n                `Order_Item_Profit_Ratio` String,\n                `Order_Item_Quantity` String,\n                Sales String,\n                `Order_Item_Total` String,\n                `Order_Profit_Per_Order` String,\n                `Order_Region` String,\n                `Order_State` String,\n                `Order_Status` String,\n                `Order_Zipcode` String,\n                `Product_Card_Id` String,\n                `Product_Category_Id` String,\n                `Product_Description` String,\n                `Product_Image` String,\n                `Product_Name` String,\n                `Product_Price` String,\n                `Product_Status` String,\n                `Shipping_Date_DateOrders` String,\n                `Shipping_Mode` String\n            ),\n            metadata Tuple(\n                key String,\n                partition String,\n                offset Int32,\n                time Int64,\n                topic String\n            )\n        )\n        ENGINE = MergeTree()\n        ORDER BY tuple();\n        \"\"\"\n    \n        client.command(create_table)\n\n        print(f\"Created {len(df)} rows into '{table_name}'\")\n    \n        # Insert\n        client.insert(\n            table_name,\n            df.values.tolist(),\n            #column_names=column_list\n        )\n\n        #print(f\"Inserted {len(df)} rows into supply_change_stream\")\n    \n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        raise\n    finally:\n        client.close()\n\n    # Return DataFrame untuk block berikutnya\n    return df", "file_path": "transformers/creative_glade.py", "language": "python", "type": "transformer", "uuid": "creative_glade"}, "transformers/clickhouse_run.py:transformer:python:clickhouse run": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.bigquery import BigQuery\nfrom os import path\nfrom pandas import DataFrame\nimport pyspark \nimport os \n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef transform_in_bigquery(*args, **kwargs) -> DataFrame:\n    \"\"\"\n    Performs a transformation in Database\n    \"\"\"\n    import sys\n    import os\n\n    # Tambahkan path untuk bisa import script eksternal\n    sys.path.append('/home/src/dataeng-2/shared/scripts')\n    sys.path.append('/home/src/dataeng-2/shared/config')\n\n    from clickhouse_raws import run_clickhouse_transformation\n\n    # Jalankan script OLAP dan ambil salah satu dataframe\n    dataframes = run_clickhouse_transformation()  # \u2190 pastikan fungsi ini mengembalikan dict DataFrame\n    return dataframes\n\n", "file_path": "transformers/clickhouse_run.py", "language": "python", "type": "transformer", "uuid": "clickhouse_run"}, "transformers/write_to_duckdb.py:transformer:python:write to duckdb": {"content": "import duckdb\nimport pandas as pd\nimport os\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n@transformer\ndef transform(data, *args, **kwargs) -> pd.DataFrame:\n    # If the input is a list of dicts, convert it to a DataFrame.\n    if isinstance(data, list):\n        df = pd.DataFrame(data)\n    else:\n        df = data  # assumed to be in the form of a DataFrame\n\n    \n    db_path = '/home/src/dataeng-2/dbt/warehouse.duckdb'\n    table_name = 'supply_chain_data'\n\n    #con = duckdb.connect(db_path, config={'allow_concurrent_connections': True})\n    con = duckdb.connect(db_path)\n  \n    # Create a table if it doesn't exist yet\n    con.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} AS\n        SELECT * FROM df LIMIT 0\n    \"\"\")  # Note: you can also use safer CREATE logic\n\n    # Insert data\n    # con.execute(\"INSERT INTO supply_chain_data SELECT * FROM df\")\n    df.to_parquet(\"/tmp/batch.parquet\")\n    con.execute(f\"COPY {table_name} FROM '/tmp/batch.parquet' (FORMAT PARQUET)\")\n\n    os.chmod(db_path, 0o666)\n    \n    con.close()\n", "file_path": "transformers/write_to_duckdb.py", "language": "python", "type": "transformer", "uuid": "write_to_duckdb"}, "transformers/run_duckdb.py:transformer:python:run duckdb": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.bigquery import BigQuery\nfrom os import path\nfrom pandas import DataFrame\nimport pyspark \nimport os \n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef transform_in_bigquery(*args, **kwargs) -> DataFrame:\n    \"\"\"\n    Performs a transformation in Database\n    \"\"\"\n    import sys\n    import os\n\n    # Tambahkan path untuk bisa import script eksternal\n    sys.path.append('/home/src/dataeng-2/shared/scripts')\n    sys.path.append('/home/src/dataeng-2/shared/config')\n\n    from duckdb_transformation import run_duckdb_transformation\n\n    # Jalankan script OLAP dan ambil salah satu dataframe\n    dataframes = run_duckdb_transformation()  # \u2190 pastikan fungsi ini mengembalikan dict DataFrame\n    return dataframes\n\n", "file_path": "transformers/run_duckdb.py", "language": "python", "type": "transformer", "uuid": "run_duckdb"}, "transformers/astonishing_cedar.py:transformer:python:astonishing cedar": {"content": "@transformer\ndef chunk_document(data, **kwargs):\n    \"\"\"\n    Split the cleaned document into manageable chunks with overlap\n    \"\"\"\n    # Check if data is available\n    if data is None or \"text\" not in data:\n        raise Exception(\"No cleaned document text available to chunk\")\n\n    text = data[\"text\"]\n    metadata = data[\"metadata\"]\n    \n    print(\"Chunking document with fixed size approach...\")\n    \n    # Set chunking parameters\n    chunk_size = 4000  # characters per chunk\n    overlap = 500      # overlap between chunks to maintain context\n    \n    # Calculate how many chunks we'll need\n    doc_length = len(text)\n    estimated_chunks = (doc_length // (chunk_size - overlap)) + 1\n    print(f\"Document length: {doc_length} characters\")\n    print(f\"Will create approximately {estimated_chunks} chunks\")\n    \n    # Create chunks with simple fixed-size approach\n    chunks = []\n    \n    # Special case: if text is smaller than chunk_size, just use one chunk\n    if doc_length <= chunk_size:\n        chunks.append({\n            \"chunk_id\": 0,\n            \"text\": text,\n            \"start_char\": 0,\n            \"end_char\": doc_length,\n            \"character_count\": doc_length,\n            \"doc_id\": metadata['source_url']\n        })\n    else:\n        # Use a straightforward loop with explicit indices\n        chunk_id = 0\n        for i in range(0, doc_length, chunk_size - overlap):\n            # Calculate chunk boundaries\n            start = i\n            end = min(i + chunk_size, doc_length)\n            \n            # Don't create tiny chunks at the end\n            if end - start < 200 and chunk_id > 0:\n                break\n                \n            # Extract chunk text\n            chunk_text = text[start:end]\n            \n            # Create chunk with metadata\n            chunks.append({\n                \"chunk_id\": chunk_id,\n                \"text\": chunk_text,\n                \"start_char\": start,\n                \"end_char\": end,\n                \"character_count\": len(chunk_text),\n                \"doc_id\": metadata['source_url']\n            })\n            \n            chunk_id += 1\n            \n            # Log progress for larger documents\n            if chunk_id % 5 == 0:\n                print(f\"Created {chunk_id} chunks so far...\")\n    \n    # Create result\n    result = {\n        \"chunks\": chunks,\n        \"chunk_count\": len(chunks),\n        \"total_characters\": doc_length,\n        \"metadata\": metadata\n    }\n    \n    print(f\"Document processed into {len(chunks)} chunks\")\n    return result", "file_path": "transformers/astonishing_cedar.py", "language": "python", "type": "transformer", "uuid": "astonishing_cedar"}, "transformers/write_to_clickhouse.py:transformer:python:write to clickhouse": {"content": "from clickhouse_driver import Client\nimport pandas as pd\nfrom datetime import datetime\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n@transformer\ndef transform(data, *args, **kwargs) -> pd.DataFrame:\n    # Konversi input ke DataFrame jika perlu\n    if isinstance(data, list):\n        df = pd.DataFrame(data)\n    else:\n        df = data.copy()\n\n    df.head(5)\n\n    # Konversi tipe data dan format tanggal\n    #df['order date (DateOrders)'] = pd.to_datetime(df['order date (DateOrders)'], format='%m/%d/%Y %H:%M')\n    #df['shipping date (DateOrders)'] = pd.to_datetime(df['shipping date (DateOrders)'], format='%m/%d/%Y %H:%M')\n    \n    # Untuk kolom numeric yang masih berupa string\n    numeric_cols = [\n        'Days for shipping (real)', 'Days for shipment (scheduled)',\n        'Benefit per order', 'Sales per customer', 'Late_delivery_risk',\n        'Category Id', 'Customer Id', 'Department Id', 'Order Customer Id',\n        'Order Id', 'Order Item Cardprod Id', 'Order Item Discount',\n        'Order Item Discount Rate', 'Order Item Id', 'Order Item Product Price',\n        'Order Item Profit Ratio', 'Order Item Quantity', 'Sales',\n        'Order Item Total', 'Order Profit Per Order', 'Product Card Id',\n        'Product Category Id', 'Product Price', 'Product Status'\n    ]\n    \n    for col in numeric_cols:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n\n    import clickhouse_connect\n\n    # Koneksi ke ClickHouse\n    con = clickhouse_connect.get_client(\n        host='clickhouse-server',  # ganti dengan IP ClickHouse Anda\n        port=8123,               # default HTTP port ClickHouse\n        username='etl_user',      # ganti jika pakai user lain\n        password='your_strong_password_123',             # isi password jika ada\n        database='etl'\n    )\n\n    table_name = 'supply_chain_stream'\n\n    # Buat tabel dengan skema yang sesuai\n    con.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            `Type` String,\n            `Days for shipping (real)` UInt8,\n            `Days for shipment (scheduled)` UInt8,\n            `Benefit per order` Float32,\n            `Sales per customer` Float32,\n            `Delivery Status` String,\n            `Late_delivery_risk` UInt8,\n            `Category Id` UInt32,\n            `Category Name` String,\n            `Customer City` String,\n            `Customer Country` String,\n            `Customer Email` String,\n            `Customer Fname` String,\n            `Customer Id` UInt32,\n            `Customer Lname` String,\n            `Customer Password` String,\n            `Customer Segment` String,\n            `Customer State` String,\n            `Customer Street` String,\n            `Customer Zipcode` String,\n            `Department Id` UInt32,\n            `Department Name` String,\n            `Latitude` Float64,\n            `Longitude` Float64,\n            `Market` String,\n            `Order City` String,\n            `Order Country` String,\n            `Order Customer Id` UInt32,\n            `order date (DateOrders)` DateTime,\n            `Order Id` UInt32,\n            `Order Item Cardprod Id` UInt32,\n            `Order Item Discount` Float32,\n            `Order Item Discount Rate` Float32,\n            `Order Item Id` UInt32,\n            `Order Item Product Price` Float32,\n            `Order Item Profit Ratio` Float32,\n            `Order Item Quantity` UInt8,\n            `Sales` Float32,\n            `Order Item Total` Float32,\n            `Order Profit Per Order` Float32,\n            `Order Region` String,\n            `Order State` String,\n            `Order Status` String,\n            `Order Zipcode` String,\n            `Product Card Id` UInt32,\n            `Product Category Id` UInt32,\n            `Product Description` String,\n            `Product Image` String,\n            `Product Name` String,\n            `Product Price` Float32,\n            `Product Status` UInt8,\n            `shipping date (DateOrders)` DateTime,\n            `Shipping Mode` String,\n            \n            -- Kafka metadata fields\n            `metadata__topic` String,\n            `metadata__partition` UInt32,\n            `metadata__offset` UInt64,\n            `metadata__timestamp` DateTime\n        ) ENGINE = MergeTree()\n        ORDER BY (`order date (DateOrders)`, `Order Id`, `Order Item Id`)\n    \"\"\")\n\n    # Konversi data ke format yang sesuai untuk ClickHouse\n    records = df.to_dict('records')\n    \n    # Insert data\n    con.execute(\n        f\"INSERT INTO {table_name} VALUES\",\n        records,\n        types_check=True\n    )\n\n    con.close()\n    return df", "file_path": "transformers/write_to_clickhouse.py", "language": "python", "type": "transformer", "uuid": "write_to_clickhouse"}, "transformers/create_table.py:transformer:python:create table": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n@transformer\ndef execute(**kwargs):\n    import clickhouse_connect\n    # Koneksi ke ClickHouse\n    client = clickhouse_connect.get_client(\n        host='clickhouse-server',  # ganti dengan IP ClickHouse Anda\n        port=8123,               # default HTTP port ClickHouse\n        username='etl_user',      # ganti jika pakai user lain\n        password='your_strong_password_123',             # isi password jika ada\n        database='etl'\n    )\n\n    # Query untuk membuat tabel\n    create_table_query = \"\"\"\n    CREATE TABLE IF NOT EXISTS test_table (\n        id UInt32,\n        name String,\n        created_at DateTime DEFAULT now()\n    ) \n    ENGINE = MergeTree\n    ORDER BY id\n    \"\"\"\n\n    # Eksekusi query\n    client.command(create_table_query)\n    print(\"Table 'test_table' created successfully in ClickHouse.\")\n", "file_path": "transformers/create_table.py", "language": "python", "type": "transformer", "uuid": "create_table"}, "transformers/wtd.py:transformer:python:wtd": {"content": "from typing import Dict, List\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef transform(messages: List[Dict], *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Args:\n        messages: List of messages in the stream.\n\n    Returns:\n        Transformed messages\n    \"\"\"\n    # Specify your transformation logic here\n\n    return messages\n", "file_path": "transformers/wtd.py", "language": "python", "type": "transformer", "uuid": "wtd"}, "pipelines/duckdb_03_dbt_run/__init__.py:pipeline:python:duckdb 03 dbt run/  init  ": {"content": "", "file_path": "pipelines/duckdb_03_dbt_run/__init__.py", "language": "python", "type": "pipeline", "uuid": "duckdb_03_dbt_run/__init__"}, "pipelines/duckdb_03_dbt_run/metadata.yaml:pipeline:yaml:duckdb 03 dbt run/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt:\n      command: run\n    dbt_profile_target: dev\n    dbt_project_name: dbt/business_transformations\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/green_myth.yaml\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: green myth\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: green_myth\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-08-09 12:44:57.119420+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: duckdb 03 dbt run\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: duckdb_03_dbt_run\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/duckdb_03_dbt_run/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "duckdb_03_dbt_run/metadata"}, "pipelines/export_to_big_query/__init__.py:pipeline:python:export to big query/  init  ": {"content": "", "file_path": "pipelines/export_to_big_query/__init__.py", "language": "python", "type": "pipeline", "uuid": "export_to_big_query/__init__"}, "pipelines/export_to_big_query/metadata.yaml:pipeline:yaml:export to big query/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - customer_dim_to_big_query\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_stg_customer\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_stg_customer\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: customer_dim_to_big_query\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_stg_customer\n  uuid: customer_dim_to_big_query\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/load_stg_department.py\n  downstream_blocks:\n  - department_dim_to_big_query\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_stg_department\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_stg_department\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/department_dim_to_big_query.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: department_dim_to_big_query\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_stg_department\n  uuid: department_dim_to_big_query\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/load_stg_location.py\n  downstream_blocks:\n  - location_dim_to_big_query\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_stg_location\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_stg_location\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/location_dim_to_big_query.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: location_dim_to_big_query\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_stg_location\n  uuid: location_dim_to_big_query\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/metadata_dim_to_big_query.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: metadata_dim_to_big_query\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_metadata_dimensio\n  uuid: metadata_dim_to_big_query\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/load_metadata_dimensio.py\n  downstream_blocks:\n  - metadata_dim_to_big_query\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_metadata_dimensio\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_metadata_dimensio\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/load_stg_order.py\n  downstream_blocks:\n  - order_dim_to_big_query\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_stg_order\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_stg_order\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/order_dim_to_big_query.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: order_dim_to_big_query\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_stg_order\n  uuid: order_dim_to_big_query\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/load_stg_product.py\n  downstream_blocks:\n  - product_dim_to_big_query\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_stg_product\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_stg_product\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/product_dim_to_big_query.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: product_dim_to_big_query\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_stg_product\n  uuid: product_dim_to_big_query\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/load_stg_shipping.py\n  downstream_blocks:\n  - shipping_dim_to_big_query\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_stg_shipping\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_stg_shipping\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/shipping_dim_to_big_query.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: shipping_dim_to_big_query\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_stg_shipping\n  uuid: shipping_dim_to_big_query\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-03-14 10:14:17.029096+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: export_to_big_query\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: export_to_big_query\nvariables:\n  stg_customer: stg_customer\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/export_to_big_query/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "export_to_big_query/metadata"}, "pipelines/export_to_big_query/triggers.yaml:pipeline:yaml:export to big query/triggers": {"content": "triggers:\n- &id002\n  envs: []\n  last_enabled_at: 2024-03-15 06:45:34.914093\n  name: trigger_export_to_bigquery\n  pipeline_uuid: export_to_big_query\n  schedule_interval: null\n  schedule_type: api\n  settings: null\n  sla: null\n  start_time: 2024-03-15 06:43:00\n  status: inactive\n  variables:\n    stg_customer: stg_customer\n- &id001\n  envs: []\n  last_enabled_at: 2024-03-15 07:28:32.464849\n  name: trigger_export_to_big_query\n  pipeline_uuid: export_to_big_query\n  schedule_interval: null\n  schedule_type: api\n  settings: null\n  sla: null\n  start_time: 2024-03-15 06:43:00\n  status: inactive\n  variables:\n    stg_customer: stg_customer\n- *id001\n- *id002\n", "file_path": "pipelines/export_to_big_query/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "export_to_big_query/triggers"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - fill_in_missing_values\n  executor_config: null\n  executor_type: local_python\n  has_callback: null\n  language: python\n  name: load_titanic\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_titanic_clean\n  executor_config: null\n  executor_type: local_python\n  has_callback: null\n  language: python\n  name: fill_in_missing_values\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: null\n  language: python\n  name: export_titanic_clean\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: null\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: example_pipeline\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: example_pipeline\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/clickhouse_01_export_duckdb_to_clickhouse/__init__.py:pipeline:python:clickhouse 01 export duckdb to clickhouse/  init  ": {"content": "", "file_path": "pipelines/clickhouse_01_export_duckdb_to_clickhouse/__init__.py", "language": "python", "type": "pipeline", "uuid": "clickhouse_01_export_duckdb_to_clickhouse/__init__"}, "pipelines/clickhouse_01_export_duckdb_to_clickhouse/metadata.yaml:pipeline:yaml:clickhouse 01 export duckdb to clickhouse/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/load_duckdb_mart_tables.py\n    file_source:\n      path: data_loaders/load_duckdb_mart_tables.py\n  downstream_blocks:\n  - export_mart_tables_to_clickhouse\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load duckdb mart tables\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_duckdb_mart_tables\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_exporters/export_mart_tables_to_clickhouse.py\n    file_source:\n      path: data_exporters/export_mart_tables_to_clickhouse.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export mart tables to clickhouse\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_duckdb_mart_tables\n  uuid: export_mart_tables_to_clickhouse\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-08-09 07:03:45.443840+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: clickhouse 01 export duckdb to clickhouse\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: clickhouse_01_export_duckdb_to_clickhouse\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/clickhouse_01_export_duckdb_to_clickhouse/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "clickhouse_01_export_duckdb_to_clickhouse/metadata"}, "pipelines/create_test_table_clickhouse/__init__.py:pipeline:python:create test table clickhouse/  init  ": {"content": "", "file_path": "pipelines/create_test_table_clickhouse/__init__.py", "language": "python", "type": "pipeline", "uuid": "create_test_table_clickhouse/__init__"}, "pipelines/create_test_table_clickhouse/metadata.yaml:pipeline:yaml:create test table clickhouse/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - create_table_2\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: create table\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks: []\n  uuid: create_table\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: create table 2\n  retry_config: null\n  status: failed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - create_table\n  uuid: create_table_2\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-08-09 00:13:57.837920+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: create test table clickhouse\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: create_test_table_clickhouse\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/create_test_table_clickhouse/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "create_test_table_clickhouse/metadata"}, "pipelines/duckdb_01_kafka_to_supply_chain_stream/__init__.py:pipeline:python:duckdb 01 kafka to supply chain stream/  init  ": {"content": "", "file_path": "pipelines/duckdb_01_kafka_to_supply_chain_stream/__init__.py", "language": "python", "type": "pipeline", "uuid": "duckdb_01_kafka_to_supply_chain_stream/__init__"}, "pipelines/duckdb_01_kafka_to_supply_chain_stream/metadata.yaml:pipeline:yaml:duckdb 01 kafka to supply chain stream/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - write_to_duckdb\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: consume_from_kafka\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: consume_from_kafka\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: transformers/write_to_duckdb.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: write_to_duckdb\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - consume_from_kafka\n  uuid: write_to_duckdb\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-03-14 05:26:11.905424+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: duckdb 01 kafka_to_supply_chain_stream\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: duckdb_01_kafka_to_supply_chain_stream\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/duckdb_01_kafka_to_supply_chain_stream/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "duckdb_01_kafka_to_supply_chain_stream/metadata"}, "pipelines/meet_attractor/__init__.py:pipeline:python:meet attractor/  init  ": {"content": "", "file_path": "pipelines/meet_attractor/__init__.py", "language": "python", "type": "pipeline", "uuid": "meet_attractor/__init__"}, "pipelines/meet_attractor/metadata.yaml:pipeline:yaml:meet attractor/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - amazing_noble\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: frosty sorcerer\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: frosty_sorcerer\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - astonishing_cedar\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: amazing noble\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - frosty_sorcerer\n  uuid: amazing_noble\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: astonishing cedar\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - amazing_noble\n  uuid: astonishing_cedar\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-08-08 03:37:20.221340+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Meet Attractor\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: meet_attractor\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/meet_attractor/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "meet_attractor/metadata"}, "pipelines/duckdb_to_clickhouse_sample/__init__.py:pipeline:python:duckdb to clickhouse sample/  init  ": {"content": "", "file_path": "pipelines/duckdb_to_clickhouse_sample/__init__.py", "language": "python", "type": "pipeline", "uuid": "duckdb_to_clickhouse_sample/__init__"}, "pipelines/duckdb_to_clickhouse_sample/metadata.yaml:pipeline:yaml:duckdb to clickhouse sample/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_data_to_clickhouse\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_data_from_duckdb\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_data_from_duckdb\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_data_to_clickhouse\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_data_from_duckdb\n  uuid: export_data_to_clickhouse\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-08-09 06:24:37.644379+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: duckdb to clickhouse sample\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: duckdb_to_clickhouse_sample\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/duckdb_to_clickhouse_sample/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "duckdb_to_clickhouse_sample/metadata"}, "pipelines/duckdb_02_supply_chain_stream_to_raws_duckdb/__init__.py:pipeline:python:duckdb 02 supply chain stream to raws duckdb/  init  ": {"content": "", "file_path": "pipelines/duckdb_02_supply_chain_stream_to_raws_duckdb/__init__.py", "language": "python", "type": "pipeline", "uuid": "duckdb_02_supply_chain_stream_to_raws_duckdb/__init__"}, "pipelines/duckdb_02_supply_chain_stream_to_raws_duckdb/metadata.yaml:pipeline:yaml:duckdb 02 supply chain stream to raws duckdb/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: transformers/insert__raws_tables_duckdb.py\n    file_source:\n      path: transformers/insert__raws_tables_duckdb.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: insert  raws tables duckdb\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks: []\n  uuid: insert__raws_tables_duckdb\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-08-04 03:54:31.912741+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: duckdb 02 supply_chain_stream_to_raws_duckdb\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: duckdb_02_supply_chain_stream_to_raws_duckdb\nvariables_dir: /home/src/mage_data/dataeng-2\nwidgets: []\n", "file_path": "pipelines/duckdb_02_supply_chain_stream_to_raws_duckdb/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "duckdb_02_supply_chain_stream_to_raws_duckdb/metadata"}}, "custom_block_template": {"custom_templates/blocks/consume_from_kafka:data_loader:yaml": {"block_type": "data_loader", "color": null, "configuration": null, "description": null, "language": "yaml", "name": null, "pipeline": {}, "tags": [], "user": {}, "template_uuid": "consume_from_kafka", "uuid": "custom_templates/blocks/consume_from_kafka", "content": "connector_type: kafka\nbootstrap_server: \"kafka:29092\"\ntopic: supply_chain_data\nconsumer_group: mage_consumer_group\ninclude_metadata: true\napi_version: \"0.10.2\"\n"}}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}